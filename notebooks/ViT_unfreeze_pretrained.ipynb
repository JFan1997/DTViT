{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms,models,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize,\n",
    "                                    RandomRotation,\n",
    "                                    RandomResizedCrop,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomAdjustSharpness,\n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "# image_mean, image_std = processor.image_mean, processor.image_std\n",
    "# height = processor.size[\"height\"]\n",
    "# width = processor.size[\"width\"]\n",
    "# size = (height, width)\n",
    "# print(\"Size: \", size)\n",
    "# print(\"Image mean: \", image_mean)\n",
    "# print(\"Image std: \", image_std)\n",
    "\n",
    "normalize = Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            transforms.CenterCrop(224), # 然后进行中心裁剪到模型期望的尺寸\n",
    "            RandomRotation(15),\n",
    "            RandomAdjustSharpness(2),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            transforms.CenterCrop(224), # 然后进行中心裁剪到模型期望的尺寸\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    for item in examples:\n",
    "        item['pixel_values'] = _train_transforms(item['image'])\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    for item in examples:\n",
    "        item['pixel_values'] = _val_transforms(item['image'])\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir,test_frac=0.15,section=\"training\"):\n",
    "        self.num_class = 0\n",
    "        self.test_frac = test_frac\n",
    "        self.section=section\n",
    "        self.transform=train_transforms if self.section==\"training\" else val_transforms\n",
    "        self.generate_data_list(data_dir)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def generate_data_list(self,data_dir):\n",
    "        # 类别名 [yes,no]\n",
    "        class_names = sorted(f\"{x.name}\" for x in Path(data_dir).iterdir() if x.is_dir())  # folder name as the class name\n",
    "        print(class_names)\n",
    "        # 2\n",
    "        self.num_class = len(class_names)\n",
    "        image_files_list = []\n",
    "        image_class = []\n",
    "        # [[class1图片列表][class2图片列表]]\n",
    "        image_files = [[f\"{x}\" for x in (Path(data_dir) / class_names[i]).iterdir()]*2 for i in range(self.num_class)]\n",
    "        # [155 yes, 98 no]\n",
    "        num_each = [len(image_files[i]) for i in range(self.num_class)]\n",
    "        class_name = []\n",
    "        # 读取所有图片为一个二维list [[class1图片列表][class2图片列表]]\n",
    "        # 对于每一类图片\n",
    "        for i in range(self.num_class):\n",
    "            # 将图片列表合并 [[class1图片列表][class2图片列表]] -> [class1图片列表+class2图片列表]\n",
    "            image_files_list.extend(image_files[i])\n",
    "            # 为每个图片标记类别，类别标签从0开始，记录index [0,0,0,1,1,1]\n",
    "            image_class.extend([i] * num_each[i])\n",
    "            # 为每个图片标记类别名 [yes,yes,yes,no,no,no]\n",
    "            class_name.extend([class_names[i]] * num_each[i])\n",
    "        length = len(image_files_list)\n",
    "        # 生成图片索引 [0,1,2,3,4,5]\n",
    "        indices = np.arange(length)\n",
    "        # 打乱图片顺序\n",
    "        np.random.shuffle(indices)\n",
    "        test_length = int(length * self.test_frac)\n",
    "        if self.section == \"test\":\n",
    "            section_indices = indices[:test_length]\n",
    "        elif self.section == \"training\":\n",
    "            section_indices = indices[test_length:]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Unsupported section: {self.section}, available options are [\"training\", \"validation\", \"test\"].'\n",
    "            )\n",
    "        # 返回数据集\n",
    "        # {\"image\":[]\n",
    "        # \"label\":[]}\n",
    "        def convert_image(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode != 'RGB':\n",
    "                image=image.convert('RGB')\n",
    "            return image\n",
    "        self.data=[{\"image\":convert_image(image_files_list[i]),\"label\": image_class[i]}  for i in section_indices ]\n",
    "        self.data=self.transform(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return self.data[index]\n",
    "        img=self.data[index][\"pixel_values\"]\n",
    "        label=self.data[index]['label']\n",
    "        # return {'pixel_values':img,'label':label}\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'yes']\n",
      "['no', 'yes']\n"
     ]
    }
   ],
   "source": [
    "data_dir='/home/jialiangfan/DTViT/dataset'\n",
    "train_dataset = MyDataset(data_dir,test_frac=0.15,section=\"training\")\n",
    "test_dataset=MyDataset(data_dir,test_frac=0.15,section=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(list(train_dataset),batch_size=4)\n",
    "test_dataloader = DataLoader(list(test_dataset),batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = models.vit_b_16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1000])\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "input=torch.randn(4,3,224,224).to(device)\n",
    "output=model(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fc_layer = model.heads\n",
    "print(fc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "    # param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 修改全连接层\n",
    "model.heads = nn.Linear(in_features=768, out_features=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数 class_token 没有被冻结，将会更新。\n",
      "参数 conv_proj.weight 没有被冻结，将会更新。\n",
      "参数 conv_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.pos_embedding 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_0.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_1.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_2.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_3.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_4.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_5.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_6.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_7.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_8.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_9.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_10.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.ln_1.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.ln_1.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.self_attention.in_proj_weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.self_attention.in_proj_bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.self_attention.out_proj.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.self_attention.out_proj.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.ln_2.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.ln_2.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.mlp.0.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.mlp.0.bias 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.mlp.3.weight 没有被冻结，将会更新。\n",
      "参数 encoder.layers.encoder_layer_11.mlp.3.bias 没有被冻结，将会更新。\n",
      "参数 encoder.ln.weight 没有被冻结，将会更新。\n",
      "参数 encoder.ln.bias 没有被冻结，将会更新。\n",
      "参数 heads.weight 没有被冻结，将会更新。\n",
      "参数 heads.bias 没有被冻结，将会更新。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历模型中的所有参数\n",
    "for name, param in model.named_parameters():\n",
    "    # 检查参数是否被冻结\n",
    "    if param.requires_grad:\n",
    "        print(f\"参数 {name} 没有被冻结，将会更新。\")\n",
    "    else:\n",
    "        print(f\"参数 {name} 被冻结，不会更新。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 running\n",
      "[Train #0] Loss: 0.1484 Acc: 94.3107% Time: 13.5803s\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 0], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 0, 0], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 0, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 0], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([0, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 0, 1, 1], device='cuda:3')\n",
      "this is the pred tensor([1, 1, 0, 1], device='cuda:3')\n",
      "this is the pred tensor([1], device='cuda:3')\n",
      "[Test #0] Loss: 0.0297 Acc: 98.7552% Time: 14.2816s\n",
      "Epoch 1 running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     27\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 10   #(set no of epochs)\n",
    "start_time = time.time() #(for showing time)\n",
    "for epoch in range(num_epochs): #(loop for every epoch)\n",
    "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()    #(training model)\n",
    "    running_loss = 0.   #(set loss 0)\n",
    "    running_corrects = 0 \n",
    "    # load a batch data of images\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # input=inputs['pixel_values']\n",
    "        # label=labels['label']\n",
    "        # print(inputs,labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # get loss value and update the network weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
    "    \n",
    "    \"\"\" Testing Phase \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            print(\"this is the pred\",preds)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(test_dataset)\n",
    "        epoch_acc = running_corrects / len(test_dataset) * 100.\n",
    "        print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'vit_unfreeze_pretrained.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvit_b_16(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)   \u001b[38;5;66;03m#load resnet18 model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#(num_of_class == 2)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(save_path))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.vit_b_16(pretrained=False)   #load resnet18 model\n",
    "model.heads = nn.Linear(768, 2)#(num_of_class == 2)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "class_names = ['no', 'yes']\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 60\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "def imshow(input, title):\n",
    "    # torch.Tensor => numpy\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # undo image normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # display images\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "##Testing\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        if i == 0:\n",
    "            print('======>RESULTS<======')\n",
    "            images = torchvision.utils.make_grid(inputs[:4])\n",
    "            imshow(images.cpu(), title=[class_names[x] for x in labels[:4]])\n",
    "    epoch_loss = running_loss / len(test_dataset)\n",
    "    epoch_acc = running_corrects / len(test_dataset) * 100.\n",
    "    print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.\n",
    "          format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
