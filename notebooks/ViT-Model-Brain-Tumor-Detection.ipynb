{"cells":[{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file preprocessor_config.json from cache at /home/jialiangfan/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\n","size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n","Image processor ViTImageProcessor {\n","  \"_valid_processor_keys\": [\n","    \"images\",\n","    \"do_resize\",\n","    \"size\",\n","    \"resample\",\n","    \"do_rescale\",\n","    \"rescale_factor\",\n","    \"do_normalize\",\n","    \"image_mean\",\n","    \"image_std\",\n","    \"return_tensors\",\n","    \"data_format\",\n","    \"input_data_format\"\n","  ],\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n"]}],"source":["from transformers import ViTImageProcessor\n","\n","processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Size:  (224, 224)\n","Image mean:  [0.5, 0.5, 0.5]\n","Image std:  [0.5, 0.5, 0.5]\n"]}],"source":["from torchvision.transforms import (CenterCrop, \n","                                    Compose, \n","                                    Normalize,\n","                                    RandomRotation,\n","                                    RandomResizedCrop,\n","                                    RandomHorizontalFlip,\n","                                    RandomAdjustSharpness,\n","                                    Resize, \n","                                    ToTensor)\n","\n","image_mean, image_std = processor.image_mean, processor.image_std\n","height = processor.size[\"height\"]\n","width = processor.size[\"width\"]\n","size = (height, width)\n","print(\"Size: \", size)\n","print(\"Image mean: \", image_mean)\n","print(\"Image std: \", image_std)\n","\n","normalize = Normalize(mean=image_mean, std=image_std)\n","_train_transforms = Compose(\n","        [\n","            Resize(size),\n","            RandomRotation(15),\n","            RandomAdjustSharpness(2),\n","            ToTensor(),\n","            normalize,\n","        ]\n","    )\n","\n","_val_transforms = Compose(\n","        [\n","            Resize(size),\n","            ToTensor(),\n","            normalize,\n","        ]\n","    )\n","\n","def train_transforms(image):\n","        return _train_transforms(image)\n","    # for item in examples:\n","        # item['pixel_values'] = _train_transforms(item['image'])\n","    # return examples\n","\n","def val_transforms(image):\n","        return _val_transforms(image)\n","    # for item in examples:\n","        # item['pixel_values'] = _val_transforms(item['image'])\n","    # return examples"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# from torch.utils.data import DataLoader\n","# from torch.utils.data import Dataset\n","# from pathlib import Path\n","# import torch\n","# import numpy as np\n","# from PIL import Image\n","# class MyDataset(Dataset):\n","#     def __init__(self, data_dir,test_frac=0.15,section=\"training\",data_augmentation=False):\n","#         self.num_class = 0\n","#         self.test_frac = test_frac\n","#         self.data_augmentation=data_augmentation\n","#         self.section=section\n","#         self.transform=train_transforms if self.section==\"training\" else val_transforms\n","#         self.generate_data_list(data_dir)\n","\n","\n","#     def __len__(self):\n","#         return len(self.data)\n","    \n","#     def generate_data_list(self,data_dir):\n","#         # 类别名 [yes,no]\n","#         class_names = sorted(f\"{x.name}\" for x in Path(data_dir).iterdir() if x.is_dir())  # folder name as the class name\n","#         # 2\n","#         self.num_class = len(class_names)\n","#         image_files_list = []\n","#         image_class = []\n","#         # [[class1图片列表][class2图片列表]]\n","#         image_files = [[f\"{x}\" for x in (Path(data_dir) / class_names[i]).iterdir()] for i in range(self.num_class)]\n","#         num_each = [len(image_files[i]) for i in range(self.num_class)]\n","#         # \n","#         max_value=max(num_each)\n","#         enlarge_factor=[max_value//num_each[i] for i in range(self.num_class)]\n","#         if not self.data_augmentation:\n","#             enlarge_factor=[1]*self.num_class\n","#         print('this is the enlarge factor',enlarge_factor)\n","\n","#         class_name = []\n","#         # 读取所有图片为一个二维list [[class1图片列表][class2图片列表]]\n","#         # 对于每一类图片\n","#         for i in range(self.num_class):\n","#             # 将图片列表合并 [[class1图片列表][class2图片列表]] -> [class1图片列表+class2图片列表]\n","#             image_files_list.extend(image_files[i]*enlarge_factor[i])\n","#             # 为每个图片标记类别，类别标签从0开始，记录index [0,0,0,1,1,1]\n","#             image_class.extend([i] * num_each[i]*enlarge_factor[i])\n","#             # 为每个图片标记类别名 [yes,yes,yes,no,no,no]\n","#             class_name.extend([class_names[i]] * num_each[i]*enlarge_factor[i])\n","#         length = len(image_files_list)\n","#         # 生成图片索引 [0,1,2,3,4,5]\n","#         indices = np.arange(length)\n","#         # 打乱图片顺序\n","#         np.random.shuffle(indices)\n","#         test_length = int(length * self.test_frac)\n","#         if self.section == \"test\":\n","#             section_indices = indices[:test_length]\n","#         elif self.section == \"training\":\n","#             section_indices = indices[test_length:]\n","#         else:\n","#             raise ValueError(\n","#                 f'Unsupported section: {self.section}, available options are [\"training\", \"validation\", \"test\"].'\n","#             )\n","#         def convert_image(image_path):\n","#             image = Image.open(image_path)\n","#             if image.mode != 'RGB':\n","#                 image=image.convert('RGB')\n","#             return image\n","#         self.data=[{\"image\":convert_image(image_files_list[i]),\"label\": image_class[i]}  for i in section_indices ]\n","#         self.data=self.transform(self.data)\n","    \n","#     def __getitem__(self, index):\n","#         # return self.data[index]\n","#         img=self.data[index][\"pixel_values\"]\n","#         label=self.data[index]['label']\n","#         return {'pixel_values':img,'label':label}\n","#         # return img,label"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import os\n","from pathlib import Path\n","import torch\n","import numpy as np\n","from PIL import Image\n","\n","class MyDataset(Dataset):\n","    def __init__(self, data_dir,test_frac=0.15,section=\"training\",balance=False):\n","        self.num_class = 0\n","        self.test_frac = test_frac\n","        self.section=section\n","        self.transform=train_transforms if self.section==\"training\" else val_transforms\n","        self.generate_data_list(data_dir)\n","        if balance:\n","            self.balance_classes()\n","\n","\n","    def __len__(self):\n","        return len(self.samples)\n","    #\n","    def balance_classes(self):\n","        from collections import Counter\n","        # 3 暂时不分类\n","        label2_counter=Counter(x[2] for x in self.samples)\n","        print(\"this is label2 counter: \", label2_counter)\n","        max_label2_count = max(label2_counter.values())\n","        print(\"max label2 count: \", max_label2_count)\n","        # before balance\n","        print(\"total num before first balance: \", len(self.samples))\n","        print(\"Deep: \", label2_counter[0])\n","        print(\"Lobar: \", label2_counter[1])\n","        print(\"Subtentorial: \", label2_counter[2])\n","        # # 为了平衡类别，复制少数类的数据，先复制label2\n","        balanced_samples = []\n","        for key in label2_counter.keys():\n","            factor = max_label2_count // label2_counter[key]\n","            if key==3:\n","                factor=1\n","            for i in range(len(self.samples)):\n","                if self.samples[i][2] == key:\n","                    balanced_samples.extend([self.samples[i]]*factor)\n","        \n","        self.samples = balanced_samples\n","        print(\"total num after first balance: \", len(self.samples))\n","        label2_counter=Counter(x[2] for x in self.samples)\n","        print(\"counter after first balance: \", label2_counter)\n","        balanced_samples=[]\n","        label1_counter = Counter(x[1] for x in self.samples)\n","        print(\"total num before second balance: \", len(self.samples))\n","        print(\"no tumor: \", label1_counter[0])\n","        print(\"tumor: \", label1_counter[1])\n","        max_label1_count = max(label1_counter.values())\n","        print(\"max label1 count: \", max_label1_count)\n","        for label in label1_counter.keys():\n","            factor = max_label1_count // label1_counter[label]\n","        #     # 复制因子次每个类别的数据\n","            for i in range(len(self.samples)):\n","                if self.samples[i][1] == label:\n","                    balanced_samples.extend([self.samples[i]]* factor)\n","        self.samples = balanced_samples\n","        print(\"total num after balance: \", len(self.samples))\n","        label1_counter = Counter(x[1] for x in self.samples)\n","        print(\"no tumor: \", label1_counter[0])\n","        print(\"tumor: \", label1_counter[1])\n","\n","\n","    def generate_data_list(self,data_dir):\n","        # 类别名 [yes,no]\n","        # class_names = sorted(f\"{x.name}\" for x in Path(data_dir).iterdir() if x.is_dir())  # folder name as the class name\n","        no_tumor_dir = os.path.join(data_dir, 'no')\n","        no_tumor_images = [(os.path.join(no_tumor_dir, img), 0, 3) for img in os.listdir(no_tumor_dir)]\n","        yes_tumor_dir = os.path.join(data_dir, 'yes')\n","        tumor_classes = {'Deep': 0, 'Lobar': 1, 'Subtentorial': 2}\n","        yes_tumor_images = []\n","        for tumor_class, label in tumor_classes.items():\n","            class_dir = os.path.join(yes_tumor_dir, tumor_class)\n","            yes_tumor_images += [(os.path.join(class_dir, img), 1, label) for img in os.listdir(class_dir)]\n","        self.samples = no_tumor_images + yes_tumor_images\n","        # self.data=self.transform(self.data)\n","    \n","    def __getitem__(self, index):\n","        img_path, has_tumor, tumor_type = self.samples[index]\n","        image = Image.open(img_path).convert('RGB')\n","        image=self.transform(image)\n","        #filename=self.data[index]['file_name']\n","        # # return {'pixel_values':img,'label':label}\n","        # return img,label,filename\n","        return {'pixel_values':image,'label1':has_tumor,'label2':tumor_type}\n","        # return image,has_tumor, tumor_type\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:15.960437Z","iopub.status.busy":"2023-05-03T08:01:15.960103Z","iopub.status.idle":"2023-05-03T08:01:15.974647Z","shell.execute_reply":"2023-05-03T08:01:15.973396Z","shell.execute_reply.started":"2023-05-03T08:01:15.960412Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import random_split\n","data_dir='/home/jialiangfan/DTViT/dataset/'\n","dataset=MyDataset(data_dir)\n","train_size = int(0.8 * len(dataset))\n","val_size = int(0.1*len(dataset))\n","test_size=len(dataset) - train_size-val_size\n","train_dataset, val_dataset,test_dataset = random_split(dataset, [train_size,val_size,test_size])\n","# train_dataset = MyDataset(data_dir,test_frac=0.15,section=\"training\",data_augmentation=True)\n","# test_dataset=MyDataset(data_dir,test_frac=0.15,section=\"test\",data_augmentation=True)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["(10120, 1266, 1265)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset),len((test_dataset)),len(val_dataset)\n","# 155+98=253"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:33.009573Z","iopub.status.busy":"2023-05-03T08:01:33.008865Z","iopub.status.idle":"2023-05-03T08:01:33.016408Z","shell.execute_reply":"2023-05-03T08:01:33.015431Z","shell.execute_reply.started":"2023-05-03T08:01:33.009541Z"},"trusted":true},"outputs":[],"source":["# def collate_fn(examples):\n","#     pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n","#     labels = torch.tensor([example[\"label\"] for example in examples])\n","#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset,batch_size=16)\n","test_dataloader = DataLoader(test_dataset,batch_size=4)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['pixel_values', 'label1', 'label2'])\n"]}],"source":["batch = next(iter(train_dataloader))\n","print(batch.keys())\n","# batch['pixel_values'].shape,batch['label'].shape\n","\n","# print(batch['pixel_values'].shape)\n","# for k,v in batch.items():\n","  # if isinstance(v, torch.Tensor):\n","    # print(k, v.shape)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pixel_values torch.Size([4, 3, 224, 224])\n","label1 torch.Size([4])\n","label2 torch.Size([4])\n"]}],"source":["batch = next(iter(test_dataloader))\n","for k,v in batch.items():\n","  if isinstance(v, torch.Tensor):\n","    print(k, v.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Of course, we would like to know the actual class name, rather than the \n","\n","---\n","\n","integer index. We can obtain that by creating a dictionary which maps between integer indices and actual class names (id2label):"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing the data\n","\n","We will now preprocess the data. The model requires 2 things: `pixel_values` and `labels`. \n","\n","We will perform data augmentaton **on-the-fly** using HuggingFace Datasets' `set_transform` method (docs can be found [here](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=set_transform#datasets.Dataset.set_transform)). This method is kind of a lazy `map`: the transform is only applied when examples are accessed. This is convenient for tokenizing or padding text, or augmenting images at training time for example, as we will do here. "]},{"cell_type":"markdown","metadata":{},"source":["It's very easy to create a corresponding PyTorch DataLoader, like so:"]},{"cell_type":"markdown","metadata":{},"source":["## Define the model\n","\n","Here we define the model. We define a `ViTForImageClassification`, which places a linear layer ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) on top of a pre-trained `ViTModel`. The linear layer is placed on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. \n","\n","The model itself is pre-trained on ImageNet-21k, a dataset of 14 million labeled images. You can find all info of the model we are going to use [here](https://huggingface.co/google/vit-base-patch16-224-in21k).\n","\n","We also specify the number of output neurons by setting the id2label and label2id mapping, which we be added as attributes to the configuration of the model (which can be accessed as `model.config`)."]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["from transformers.models.vit.modeling_vit import * \n","\n","_IMAGE_CLASS_CHECKPOINT = \"google/vit-base-patch16-224\"\n","_CONFIG_FOR_DOC = \"ViTConfig\"\n","_IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n","\n","class DualViTForImageClassification(ViTPreTrainedModel):\n","    def __init__(self, config: ViTConfig) -> None:\n","        super().__init__(config)\n","\n","        self.num_labels = config.num_labels\n","        self.vit = ViTModel(config, add_pooling_layer=False)\n","\n","        # Classifier head\n","        self.classifier1 = nn.Linear(config.hidden_size, 2) if config.num_labels > 0 else nn.Identity()\n","        self.classifier2 = nn.Linear(config.hidden_size, 4) if config.num_labels > 0 else nn.Identity()\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    @add_start_docstrings_to_model_forward(VIT_INPUTS_DOCSTRING)\n","    @add_code_sample_docstrings(\n","        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n","        output_type=ImageClassifierOutput,\n","        config_class=_CONFIG_FOR_DOC,\n","        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n","    )\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.Tensor] = None,\n","        head_mask: Optional[torch.Tensor] = None,\n","        label1: Optional[torch.Tensor] = None,\n","        label2: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        interpolate_pos_encoding: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[tuple, ImageClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        # vit的输出是一个元组，第一个元素是最后一个token的输出，第二个元素是所有token的输出\n","        outputs = self.vit(\n","            pixel_values,\n","            head_mask=head_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            interpolate_pos_encoding=interpolate_pos_encoding,\n","            return_dict=return_dict,\n","        )\n","        sequence_output = outputs[0]\n","        logits1 = self.classifier1(sequence_output[:, 0, :])\n","        logits2 = self.classifier2(sequence_output[:, 0, :])\n","        loss = None\n","        loss_fct = CrossEntropyLoss()\n","        if label1 is not None:\n","            # move labels to correct device to enable model parallelism\n","            label1 = label1.to(logits1.device)\n","            loss1 = loss_fct(logits1.view(-1, self.num_labels), label1.view(-1))\n","        if label2 is not None:\n","            # move labels to correct device to enable model parallelism\n","            label2 = label2.to(logits2.device)\n","            loss2 = loss_fct(logits2.view(-1, self.num_labels), label2.view(-1))\n","        loss=loss1+loss2\n","        if not return_dict:\n","            output = (logits1,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return ImageClassifierOutput(\n","            loss=loss,\n","            logits=logits1,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:33.188811Z","iopub.status.busy":"2023-05-03T08:01:33.186466Z","iopub.status.idle":"2023-05-03T08:01:37.057345Z","shell.execute_reply":"2023-05-03T08:01:37.056513Z","shell.execute_reply.started":"2023-05-03T08:01:33.188778Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading weights file model.safetensors from cache at /home/jialiangfan/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/model.safetensors\n","Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing DualViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing DualViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DualViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DualViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier1.bias', 'classifier1.weight', 'classifier2.bias', 'classifier2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import ViTConfig\n","from torch import nn\n","config=ViTConfig()\n","config.problem_type=\"single_label_classification\"\n","model = DualViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\",config=config)\n","# model.classifier=nn.Linear(in_features=768, out_features=train_dataset.num_class, bias=True)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"data":{"text/plain":["Linear(in_features=768, out_features=2, bias=True)"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["model.classifier1"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"data":{"text/plain":["Linear(in_features=768, out_features=4, bias=True)"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["model.classifier2"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["# for name, param in model.named_parameters():\n","#     if name.startswith(\"classifier\"):\n","#         param.requires_grad = True\n","#     else:\n","#         model.requires_grad_=False"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["参数 vit.embeddings.cls_token 没有被冻结，将会更新。\n","参数 vit.embeddings.position_embeddings 没有被冻结，将会更新。\n","参数 vit.embeddings.patch_embeddings.projection.weight 没有被冻结，将会更新。\n","参数 vit.embeddings.patch_embeddings.projection.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.0.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.1.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.2.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.3.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.4.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.5.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.6.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.7.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.8.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.9.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.10.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.query.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.query.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.key.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.key.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.value.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.attention.value.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.attention.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.intermediate.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.intermediate.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.output.dense.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.output.dense.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.layernorm_before.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.layernorm_before.bias 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.layernorm_after.weight 没有被冻结，将会更新。\n","参数 vit.encoder.layer.11.layernorm_after.bias 没有被冻结，将会更新。\n","参数 vit.layernorm.weight 没有被冻结，将会更新。\n","参数 vit.layernorm.bias 没有被冻结，将会更新。\n","参数 classifier1.weight 没有被冻结，将会更新。\n","参数 classifier1.bias 没有被冻结，将会更新。\n","参数 classifier2.weight 没有被冻结，将会更新。\n","参数 classifier2.bias 没有被冻结，将会更新。\n"]}],"source":["\n","# 遍历模型中的所有参数\n","for name, param in model.named_parameters():\n","    # 检查参数是否被冻结\n","    if param.requires_grad:\n","        print(f\"参数 {name} 没有被冻结，将会更新。\")\n","    else:\n","        print(f\"参数 {name} 被冻结，不会更新。\")"]},{"cell_type":"markdown","metadata":{},"source":["To instantiate a `Trainer`, we will need to define three more things. The most important is the `TrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n","\n","We also set the argument \"remove_unused_columns\" to False, because otherwise the \"img\" column would be removed, which is required for the data transformations."]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.059337Z","iopub.status.busy":"2023-05-03T08:01:37.058993Z","iopub.status.idle":"2023-05-03T08:01:37.458777Z","shell.execute_reply":"2023-05-03T08:01:37.457867Z","shell.execute_reply.started":"2023-05-03T08:01:37.059303Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","PyTorch: setting up devices\n"]}],"source":["from transformers import TrainingArguments, Trainer\n","import os\n","\n","os.environ['NCCL_P2P_DISABLE'] = '1'\n","os.environ['NCCL_IB_DISABLE'] = '1'\n","\n","metric_name = \"accuracy\"\n","args = TrainingArguments(\n","    \"Brain-Tumor-Detection\",\n","    save_strategy=\"epoch\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5, #0.00002, #0.00002\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n","    logging_dir='logs',\n","    remove_unused_columns=False,\n","    report_to=\"tensorboard\",\n",")\n","# args.set_optimizer(name=\"sgd\")"]},{"cell_type":"markdown","metadata":{},"source":["Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, set the training and evaluation batch_sizes and customize the number of epochs for training, as well as the weight decay.\n","\n","We also define a `compute_metrics` function that will be used to compute metrics at evaluation. We use \"accuracy\" here.\n"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.461670Z","iopub.status.busy":"2023-05-03T08:01:37.461329Z","iopub.status.idle":"2023-05-03T08:01:37.467050Z","shell.execute_reply":"2023-05-03T08:01:37.465901Z","shell.execute_reply.started":"2023-05-03T08:01:37.461638Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return dict(accuracy=accuracy_score(predictions, labels))"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.469330Z","iopub.status.busy":"2023-05-03T08:01:37.468596Z","iopub.status.idle":"2023-05-03T08:01:42.248796Z","shell.execute_reply":"2023-05-03T08:01:42.247906Z","shell.execute_reply.started":"2023-05-03T08:01:37.469297Z"},"trusted":true},"outputs":[],"source":["import torch\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    # data_collator=collate_fn,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor,\n","    \n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train the model\n"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:42.255611Z","iopub.status.busy":"2023-05-03T08:01:42.253570Z","iopub.status.idle":"2023-05-03T08:03:20.585869Z","shell.execute_reply":"2023-05-03T08:03:20.584719Z","shell.execute_reply.started":"2023-05-03T08:01:42.255574Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 10,120\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Training with DataParallel so batch size has been adjusted to: 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6,330\n","  Number of trainable parameters = 85,803,270\n"]},{"ename":"ValueError","evalue":"Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2107629/1774055843.py\", line 67, in forward\n    loss2 = loss_fct(logits2.view(-1, self.num_labels), label2.view(-1))\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/loss.py\", line 1185, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/functional.py\", line 3086, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\nValueError: Expected input batch_size (16) to match target batch_size (8).\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m~/miniconda3/envs/medical/lib/python3.8/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2107629/1774055843.py\", line 67, in forward\n    loss2 = loss_fct(logits2.view(-1, self.num_labels), label2.view(-1))\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/modules/loss.py\", line 1185, in forward\n    return F.cross_entropy(input, target, weight=self.weight,\n  File \"/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/torch/nn/functional.py\", line 3086, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\nValueError: Expected input batch_size (16) to match target batch_size (8).\n"]}],"source":["import os\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation\n","\n","Finally, let's evaluate the model on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:03:34.592321Z","iopub.status.busy":"2023-05-03T08:03:34.591973Z","iopub.status.idle":"2023-05-03T08:03:35.460411Z","shell.execute_reply":"2023-05-03T08:03:35.457901Z","shell.execute_reply.started":"2023-05-03T08:03:34.592294Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"]}],"source":["outputs = trainer.predict(test_dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:10:50.111726Z","iopub.status.busy":"2023-05-03T08:10:50.110709Z","iopub.status.idle":"2023-05-03T08:10:50.118313Z","shell.execute_reply":"2023-05-03T08:10:50.117273Z","shell.execute_reply.started":"2023-05-03T08:10:50.111680Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'outputs' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics)\n","\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"]}],"source":["print(outputs.metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":4}
