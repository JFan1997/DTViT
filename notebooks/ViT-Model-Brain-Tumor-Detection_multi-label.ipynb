{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Fine-tune the Vision Transformer on Brain MRI Images Dataset\n","\n","\n","In this notebook, we are going to fine-tune a pre-trained [Vision Transformer](https://huggingface.co/docs/transformers/model_doc/vit) (which I added to [Transformers](https://github.com/huggingface/transformers)) on the Fashion Product Images dataset.\n","\n","We will prepare the data using [datasets](https://github.com/huggingface/datasets), and train the model using the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html). For other notebooks (such as training ViT with PyTorch Lightning), I refer to my repo [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials). \n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-03T07:59:32.751936Z","iopub.status.busy":"2023-05-03T07:59:32.749723Z","iopub.status.idle":"2023-05-03T07:59:44.839271Z","shell.execute_reply":"2023-05-03T07:59:44.838143Z","shell.execute_reply.started":"2023-05-03T07:59:32.751902Z"},"trusted":true},"outputs":[],"source":["!pip install -q transformers datasets   "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/jialiangfan/miniconda3/envs/medical/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import ViTImageProcessor\n","\n","processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Size:  (224, 224)\n","Image mean:  [0.5, 0.5, 0.5]\n","Image std:  [0.5, 0.5, 0.5]\n"]}],"source":["from torchvision.transforms import (CenterCrop, \n","                                    Compose, \n","                                    Normalize,\n","                                    RandomRotation,\n","                                    RandomResizedCrop,\n","                                    RandomHorizontalFlip,\n","                                    RandomAdjustSharpness,\n","                                    Resize, \n","                                    ToTensor)\n","\n","image_mean, image_std = processor.image_mean, processor.image_std\n","height = processor.size[\"height\"]\n","width = processor.size[\"width\"]\n","size = (height, width)\n","print(\"Size: \", size)\n","print(\"Image mean: \", image_mean)\n","print(\"Image std: \", image_std)\n","\n","normalize = Normalize(mean=image_mean, std=image_std)\n","_train_transforms = Compose(\n","        [\n","            Resize(size),\n","            RandomRotation(15),\n","            RandomAdjustSharpness(2),\n","            ToTensor(),\n","            normalize,\n","        ]\n","    )\n","\n","_val_transforms = Compose(\n","        [\n","            Resize(size),\n","            ToTensor(),\n","            normalize,\n","        ]\n","    )\n","\n","def train_transforms(examples):\n","        # return _train_transforms(image)\n","    for item in examples:\n","        item['pixel_values'] = _train_transforms(item['image'])\n","    return examples\n","\n","def val_transforms(examples):\n","    for item in examples:\n","        item['pixel_values'] = _val_transforms(item['image'])\n","    return examples"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from pathlib import Path\n","import torch\n","import numpy as np\n","from PIL import Image\n","class MyDataset(Dataset):\n","    def __init__(self, data_dir,test_frac=0.15,section=\"training\",data_augmentation=False):\n","        self.num_class = 0\n","        self.test_frac = test_frac\n","        self.data_augmentation=data_augmentation\n","        self.section=section\n","        self.transform=train_transforms if self.section==\"training\" else val_transforms\n","        self.generate_data_list(data_dir)\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def generate_data_list(self,data_dir):\n","        # 类别名 [yes,no]\n","        class_names = sorted(f\"{x.name}\" for x in Path(data_dir).iterdir() if x.is_dir())  # folder name as the class name\n","        # 2\n","        self.num_class = len(class_names)\n","        image_files_list = []\n","        image_class = []\n","        # [[class1图片列表][class2图片列表]]\n","        image_files = [[f\"{x}\" for x in (Path(data_dir) / class_names[i]).iterdir()] for i in range(self.num_class)]\n","        num_each = [len(image_files[i]) for i in range(self.num_class)]\n","        # \n","        max_value=max(num_each)\n","        enlarge_factor=[max_value//num_each[i] for i in range(self.num_class)]\n","        if not self.data_augmentation:\n","            enlarge_factor=[1]*self.num_class\n","        print('this is the enlarge factor',enlarge_factor)\n","\n","        class_name = []\n","        # 读取所有图片为一个二维list [[class1图片列表][class2图片列表]]\n","        # 对于每一类图片\n","        for i in range(self.num_class):\n","            # 将图片列表合并 [[class1图片列表][class2图片列表]] -> [class1图片列表+class2图片列表]\n","            image_files_list.extend(image_files[i]*enlarge_factor[i])\n","            # 为每个图片标记类别，类别标签从0开始，记录index [0,0,0,1,1,1]\n","            image_class.extend([i] * num_each[i]*enlarge_factor[i])\n","            # 为每个图片标记类别名 [yes,yes,yes,no,no,no]\n","            class_name.extend([class_names[i]] * num_each[i]*enlarge_factor[i])\n","        length = len(image_files_list)\n","        # 生成图片索引 [0,1,2,3,4,5]\n","        indices = np.arange(length)\n","        # 打乱图片顺序\n","        np.random.shuffle(indices)\n","        test_length = int(length * self.test_frac)\n","        if self.section == \"test\":\n","            section_indices = indices[:test_length]\n","        elif self.section == \"training\":\n","            section_indices = indices[test_length:]\n","        else:\n","            raise ValueError(\n","                f'Unsupported section: {self.section}, available options are [\"training\", \"validation\", \"test\"].'\n","            )\n","        def convert_image(image_path):\n","            image = Image.open(image_path)\n","            if image.mode != 'RGB':\n","                image=image.convert('RGB')\n","            return image\n","        self.data=[{\"image\":convert_image(image_files_list[i]),\"label\": image_class[i]}  for i in section_indices ]\n","        self.data=self.transform(self.data)\n","    \n","    def __getitem__(self, index):\n","        # return self.data[index]\n","        img=self.data[index][\"pixel_values\"]\n","        label=self.data[index]['label']\n","        return {'pixel_values':img,'label':label}\n","        # return img,label"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# from torch.utils.data import DataLoader\n","# from torch.utils.data import Dataset\n","# import os\n","# from pathlib import Path\n","# import torch\n","# import numpy as np\n","# from PIL import Image\n","\n","# class MyDataset(Dataset):\n","#     def __init__(self, data_dir,test_frac=0.15,section=\"training\",balance=False):\n","#         self.num_class = 0\n","#         self.test_frac = test_frac\n","#         self.section=section\n","#         self.transform=train_transforms if self.section==\"training\" else val_transforms\n","#         self.generate_data_list(data_dir)\n","#         if balance:\n","#             self.balance_classes()\n","\n","\n","#     def __len__(self):\n","#         return len(self.samples)\n","#     #\n","#     def balance_classes(self):\n","#         from collections import Counter\n","#         # 3 暂时不分类\n","#         label2_counter=Counter(x[2] for x in self.samples)\n","#         print(\"this is label2 counter: \", label2_counter)\n","#         max_label2_count = max(label2_counter.values())\n","#         print(\"max label2 count: \", max_label2_count)\n","#         # before balance\n","#         print(\"total num before first balance: \", len(self.samples))\n","#         print(\"Deep: \", label2_counter[0])\n","#         print(\"Lobar: \", label2_counter[1])\n","#         print(\"Subtentorial: \", label2_counter[2])\n","#         # # 为了平衡类别，复制少数类的数据，先复制label2\n","#         balanced_samples = []\n","#         for key in label2_counter.keys():\n","#             factor = max_label2_count // label2_counter[key]\n","#             if key==3:\n","#                 factor=1\n","#             for i in range(len(self.samples)):\n","#                 if self.samples[i][2] == key:\n","#                     balanced_samples.extend([self.samples[i]]*factor)\n","        \n","#         self.samples = balanced_samples\n","#         print(\"total num after first balance: \", len(self.samples))\n","#         label2_counter=Counter(x[2] for x in self.samples)\n","#         print(\"counter after first balance: \", label2_counter)\n","#         balanced_samples=[]\n","#         label1_counter = Counter(x[1] for x in self.samples)\n","#         print(\"total num before second balance: \", len(self.samples))\n","#         print(\"no tumor: \", label1_counter[0])\n","#         print(\"tumor: \", label1_counter[1])\n","#         max_label1_count = max(label1_counter.values())\n","#         print(\"max label1 count: \", max_label1_count)\n","#         for label in label1_counter.keys():\n","#             factor = max_label1_count // label1_counter[label]\n","#         #     # 复制因子次每个类别的数据\n","#             for i in range(len(self.samples)):\n","#                 if self.samples[i][1] == label:\n","#                     balanced_samples.extend([self.samples[i]]* factor)\n","#         self.samples = balanced_samples\n","#         print(\"total num after balance: \", len(self.samples))\n","#         label1_counter = Counter(x[1] for x in self.samples)\n","#         print(\"no tumor: \", label1_counter[0])\n","#         print(\"tumor: \", label1_counter[1])\n","\n","\n","#     def generate_data_list(self,data_dir):\n","#         # 类别名 [yes,no]\n","#         # class_names = sorted(f\"{x.name}\" for x in Path(data_dir).iterdir() if x.is_dir())  # folder name as the class name\n","#         no_tumor_dir = os.path.join(data_dir, 'no')\n","#         no_tumor_images = [(os.path.join(no_tumor_dir, img), 0, 3) for img in os.listdir(no_tumor_dir)]\n","#         yes_tumor_dir = os.path.join(data_dir, 'yes')\n","#         tumor_classes = {'Deep': 0, 'Lobar': 1, 'Subtentorial': 2}\n","#         yes_tumor_images = []\n","#         for tumor_class, label in tumor_classes.items():\n","#             class_dir = os.path.join(yes_tumor_dir, tumor_class)\n","#             yes_tumor_images += [(os.path.join(class_dir, img), 1, label) for img in os.listdir(class_dir)]\n","#         self.samples = no_tumor_images + yes_tumor_images\n","#         # self.data=self.transform(self.data)\n","    \n","#     def __getitem__(self, index):\n","#         img_path, has_tumor, tumor_type = self.samples[index]\n","#         image = Image.open(img_path).convert('RGB')\n","#         image=self.transform(image)\n","#         #filename=self.data[index]['file_name']\n","#         # # return {'pixel_values':img,'label':label}\n","#         # return img,label,filename\n","#         return {'pixel_values':image,'label1':has_tumor,'label2':tumor_type}\n","#         # return image,has_tumor, tumor_type\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:15.960437Z","iopub.status.busy":"2023-05-03T08:01:15.960103Z","iopub.status.idle":"2023-05-03T08:01:15.974647Z","shell.execute_reply":"2023-05-03T08:01:15.973396Z","shell.execute_reply.started":"2023-05-03T08:01:15.960412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["this is the enlarge factor [1, 3, 12, 1]\n","this is the enlarge factor [1, 3, 12, 1]\n"]}],"source":["# from torch.utils.data import random_split\n","data_dir='/home/jialiangfan/DTViT/dataset1/'\n","# dataset=MyDataset(data_dir)\n","# train_size = int(0.8 * len(datase/t))\n","    \n","# val_size = int(0.1*len(dataset))\n","# test_size=len(dataset) - train_size-val_size\n","# train_dataset, val_dataset,test_dataset = random_split(dataset, [train_size,val_size,test_size])\n","train_dataset = MyDataset(data_dir,test_frac=0.15,section=\"training\",data_augmentation=True)\n","test_dataset=MyDataset(data_dir,test_frac=0.15,section=\"test\",data_augmentation=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["18197"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)\n","# 155+98=253"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["3211"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len((test_dataset))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:33.009573Z","iopub.status.busy":"2023-05-03T08:01:33.008865Z","iopub.status.idle":"2023-05-03T08:01:33.016408Z","shell.execute_reply":"2023-05-03T08:01:33.015431Z","shell.execute_reply.started":"2023-05-03T08:01:33.009541Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","import torch\n","\n","# def collate_fn(examples):\n","#     pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n","#     labels = torch.tensor([example[\"label\"] for example in examples])\n","#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","train_dataloader = DataLoader(train_dataset,batch_size=32)\n","test_dataloader = DataLoader(test_dataset,batch_size=4)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['pixel_values', 'label'])\n"]}],"source":["batch = next(iter(train_dataloader))\n","print(batch.keys())\n","# batch['pixel_values'].shape,batch['label'].shape\n","\n","# print(batch['pixel_values'].shape)\n","# for k,v in batch.items():\n","  # if isinstance(v, torch.Tensor):\n","    # print(k, v.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["pixel_values torch.Size([4, 3, 224, 224])\n","label torch.Size([4])\n"]}],"source":["batch = next(iter(test_dataloader))\n","for k,v in batch.items():\n","  if isinstance(v, torch.Tensor):\n","    print(k, v.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Of course, we would like to know the actual class name, rather than the \n","\n","---\n","\n","integer index. We can obtain that by creating a dictionary which maps between integer indices and actual class names (id2label):"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing the data\n","\n","We will now preprocess the data. The model requires 2 things: `pixel_values` and `labels`. \n","\n","We will perform data augmentaton **on-the-fly** using HuggingFace Datasets' `set_transform` method (docs can be found [here](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=set_transform#datasets.Dataset.set_transform)). This method is kind of a lazy `map`: the transform is only applied when examples are accessed. This is convenient for tokenizing or padding text, or augmenting images at training time for example, as we will do here. "]},{"cell_type":"markdown","metadata":{},"source":["It's very easy to create a corresponding PyTorch DataLoader, like so:"]},{"cell_type":"markdown","metadata":{},"source":["## Define the model\n","\n","Here we define the model. We define a `ViTForImageClassification`, which places a linear layer ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) on top of a pre-trained `ViTModel`. The linear layer is placed on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. \n","\n","The model itself is pre-trained on ImageNet-21k, a dataset of 14 million labeled images. You can find all info of the model we are going to use [here](https://huggingface.co/google/vit-base-patch16-224-in21k).\n","\n","We also specify the number of output neurons by setting the id2label and label2id mapping, which we be added as attributes to the configuration of the model (which can be accessed as `model.config`)."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:33.188811Z","iopub.status.busy":"2023-05-03T08:01:33.186466Z","iopub.status.idle":"2023-05-03T08:01:37.057345Z","shell.execute_reply":"2023-05-03T08:01:37.056513Z","shell.execute_reply.started":"2023-05-03T08:01:33.188778Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import ViTForImageClassification,ViTConfig\n","from torch import nn\n","config=ViTConfig()\n","config.num_labels=4\n","config.problem_type=\"multi_label_classification\"\n","model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\",config=config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for name, param in model.named_parameters():\n","#     if name.startswith(\"classifier\"):\n","#         param.requires_grad = True\n","#     else:\n","#         model.requires_grad_=False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# 遍历模型中的所有参数\n","for name, param in model.named_parameters():\n","    # 检查参数是否被冻结\n","    if param.requires_grad:\n","        print(f\"参数 {name} 没有被冻结，将会更新。\")\n","    else:\n","        print(f\"参数 {name} 被冻结，不会更新。\")"]},{"cell_type":"markdown","metadata":{},"source":["To instantiate a `Trainer`, we will need to define three more things. The most important is the `TrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n","\n","We also set the argument \"remove_unused_columns\" to False, because otherwise the \"img\" column would be removed, which is required for the data transformations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.059337Z","iopub.status.busy":"2023-05-03T08:01:37.058993Z","iopub.status.idle":"2023-05-03T08:01:37.458777Z","shell.execute_reply":"2023-05-03T08:01:37.457867Z","shell.execute_reply.started":"2023-05-03T08:01:37.059303Z"},"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","import os\n","\n","os.environ['NCCL_P2P_DISABLE'] = '1'\n","os.environ['NCCL_IB_DISABLE'] = '1'\n","\n","metric_name = \"accuracy\"\n","args = TrainingArguments(\n","    \"Brain-Tumor-Detection\",\n","    save_strategy=\"epoch\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5, #0.00002, #0.00002\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n","    logging_dir='logs',\n","    remove_unused_columns=False,\n","    report_to=\"tensorboard\",\n",")\n","# args.set_optimizer(name=\"sgd\")"]},{"cell_type":"markdown","metadata":{},"source":["Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, set the training and evaluation batch_sizes and customize the number of epochs for training, as well as the weight decay.\n","\n","We also define a `compute_metrics` function that will be used to compute metrics at evaluation. We use \"accuracy\" here.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.461670Z","iopub.status.busy":"2023-05-03T08:01:37.461329Z","iopub.status.idle":"2023-05-03T08:01:37.467050Z","shell.execute_reply":"2023-05-03T08:01:37.465901Z","shell.execute_reply.started":"2023-05-03T08:01:37.461638Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return dict(accuracy=accuracy_score(predictions, labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:37.469330Z","iopub.status.busy":"2023-05-03T08:01:37.468596Z","iopub.status.idle":"2023-05-03T08:01:42.248796Z","shell.execute_reply":"2023-05-03T08:01:42.247906Z","shell.execute_reply.started":"2023-05-03T08:01:37.469297Z"},"trusted":true},"outputs":[],"source":["import torch\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    # data_collator=collate_fn,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor,\n","    \n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:01:42.255611Z","iopub.status.busy":"2023-05-03T08:01:42.253570Z","iopub.status.idle":"2023-05-03T08:03:20.585869Z","shell.execute_reply":"2023-05-03T08:03:20.584719Z","shell.execute_reply.started":"2023-05-03T08:01:42.255574Z"},"trusted":true},"outputs":[],"source":["import os\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation\n","\n","Finally, let's evaluate the model on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:03:34.592321Z","iopub.status.busy":"2023-05-03T08:03:34.591973Z","iopub.status.idle":"2023-05-03T08:03:35.460411Z","shell.execute_reply":"2023-05-03T08:03:35.457901Z","shell.execute_reply.started":"2023-05-03T08:03:34.592294Z"},"trusted":true},"outputs":[],"source":["outputs = trainer.predict(test_dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-03T08:10:50.111726Z","iopub.status.busy":"2023-05-03T08:10:50.110709Z","iopub.status.idle":"2023-05-03T08:10:50.118313Z","shell.execute_reply":"2023-05-03T08:10:50.117273Z","shell.execute_reply.started":"2023-05-03T08:10:50.111680Z"},"trusted":true},"outputs":[],"source":["print(outputs.metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":4}
