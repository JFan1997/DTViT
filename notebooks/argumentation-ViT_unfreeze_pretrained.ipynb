{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms,models,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision.models import vit_b_16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize,\n",
    "                                    RandomRotation,\n",
    "                                    RandomResizedCrop,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomAdjustSharpness,\n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "# image_mean, image_std = processor.image_mean, processor.image_std\n",
    "# height = processor.size[\"height\"]\n",
    "# width = processor.size[\"width\"]\n",
    "# size = (height, width)\n",
    "# print(\"Size: \", size)\n",
    "# print(\"Image mean: \", image_mean)\n",
    "# print(\"Image std: \", image_std)\n",
    "\n",
    "normalize = Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            transforms.CenterCrop(224), # 然后进行中心裁剪到模型期望的尺寸\n",
    "            RandomRotation(15),\n",
    "            RandomAdjustSharpness(2),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(256),\n",
    "            transforms.CenterCrop(224), # 然后进行中心裁剪到模型期望的尺寸\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(image):\n",
    "    return _train_transforms(image)\n",
    "    # for item in examples:\n",
    "        # item['pixel_values'] = _train_transforms(item['image'])\n",
    "    # return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    for item in examples:\n",
    "        item['pixel_values'] = _val_transforms(item['image'])\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from five_dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018-2021所有病人的数据\n",
    "data_dir='/home/jialiangfan/DTViT/dataset2'\n",
    "dataset=MyDataset(data_dir)\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.85 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# train_dataset = MyDataset(data_dir,test_frac=0.15,section=\"training\")\n",
    "# test_dataset=MyDataset(data_dir,test_frac=0.15,section=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_proj\n",
      "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "------------------\n",
      "encoder\n",
      "Encoder(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (encoder_layer_0): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_1): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_2): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_3): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_4): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_5): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_6): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_7): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_8): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_9): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_10): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder_layer_11): EncoderBlock(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (self_attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLPBlock(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "------------------\n",
      "heads1\n",
      "Sequential(\n",
      "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "------------------\n",
      "heads2\n",
      "Sequential(\n",
      "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DualVisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads1): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (heads2): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.transformer import DualVisionTransformer\n",
    "\n",
    "model = DualVisionTransformer(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        hidden_dim=768,\n",
    "        mlp_dim=3072,\n",
    "        dropout=0.1,\n",
    "        attention_dropout=0.1,\n",
    "        num_classe1=2,\n",
    "        num_classe2=3,\n",
    "    )\n",
    "\n",
    "for name, moudle in model.named_children():\n",
    "    print(name)\n",
    "    print(moudle)\n",
    "    print('------------------')\n",
    "# print(model.named_children)\n",
    "# aa=nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(models.transformer.DualVisionTransformer, models.transformer.Encoder)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model),type(model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion1 = nn.CrossEntropyLoss()  #(set loss function)\n",
    "criterion2 = nn.CrossEntropyLoss()  #(set loss function)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 running\n",
      "[Train #0] Loss: 0.9058 Acc: 83.5441% Time: 126.8763s\n",
      "[Test #0] Loss: 0.7507 Acc: 85.6164% Time: 136.7137s\n",
      "Epoch 1 running\n",
      "[Train #1] Loss: 0.7752 Acc: 85.6180% Time: 265.7875s\n",
      "[Test #1] Loss: 0.7482 Acc: 85.5901% Time: 275.8348s\n",
      "Epoch 2 running\n",
      "[Train #2] Loss: 0.7108 Acc: 86.5991% Time: 404.4675s\n",
      "[Test #2] Loss: 0.6688 Acc: 87.1180% Time: 414.3811s\n",
      "Epoch 3 running\n",
      "[Train #3] Loss: 0.6900 Acc: 87.0594% Time: 542.3916s\n",
      "[Test #3] Loss: 0.5754 Acc: 88.9884% Time: 552.3369s\n",
      "Epoch 4 running\n",
      "[Train #4] Loss: 0.6317 Acc: 87.9615% Time: 679.9420s\n",
      "[Test #4] Loss: 0.5208 Acc: 89.8841% Time: 689.8306s\n",
      "Epoch 5 running\n",
      "[Train #5] Loss: 0.6000 Acc: 88.5520% Time: 817.8841s\n",
      "[Test #5] Loss: 0.4599 Acc: 91.0695% Time: 827.8588s\n",
      "Epoch 6 running\n",
      "[Train #6] Loss: 0.5663 Acc: 89.3658% Time: 956.0007s\n",
      "[Test #6] Loss: 0.4917 Acc: 91.1222% Time: 965.9632s\n",
      "Epoch 7 running\n",
      "[Train #7] Loss: 0.5404 Acc: 89.8075% Time: 1094.1518s\n",
      "[Test #7] Loss: 0.4724 Acc: 91.1486% Time: 1104.0747s\n",
      "Epoch 8 running\n",
      "[Train #8] Loss: 0.5110 Acc: 90.3794% Time: 1232.7084s\n",
      "[Test #8] Loss: 0.4686 Acc: 90.8061% Time: 1242.5494s\n",
      "Epoch 9 running\n",
      "[Train #9] Loss: 0.4746 Acc: 91.0258% Time: 1370.6780s\n",
      "[Test #9] Loss: 0.4003 Acc: 92.1760% Time: 1380.6733s\n",
      "Epoch 10 running\n",
      "[Train #10] Loss: 0.4541 Acc: 91.4024% Time: 1509.4762s\n",
      "[Test #10] Loss: 0.4187 Acc: 92.2023% Time: 1519.3281s\n",
      "Epoch 11 running\n",
      "[Train #11] Loss: 0.4389 Acc: 91.6860% Time: 1647.8377s\n",
      "[Test #11] Loss: 0.4138 Acc: 92.2287% Time: 1657.7606s\n",
      "Epoch 12 running\n",
      "[Train #12] Loss: 0.4252 Acc: 92.0766% Time: 1785.0720s\n",
      "[Test #12] Loss: 0.4353 Acc: 91.8598% Time: 1794.9527s\n",
      "Epoch 13 running\n",
      "[Train #13] Loss: 0.4176 Acc: 92.2905% Time: 1924.3052s\n",
      "[Test #13] Loss: 0.3942 Acc: 92.7028% Time: 1934.1682s\n",
      "Epoch 14 running\n",
      "[Train #14] Loss: 0.3976 Acc: 92.4440% Time: 2061.9384s\n",
      "[Test #14] Loss: 0.4524 Acc: 90.9378% Time: 2071.9451s\n",
      "Epoch 15 running\n",
      "[Train #15] Loss: 0.3999 Acc: 92.5602% Time: 2200.4246s\n",
      "[Test #15] Loss: 0.4082 Acc: 92.4921% Time: 2210.3172s\n",
      "Epoch 16 running\n",
      "[Train #16] Loss: 0.3813 Acc: 92.9043% Time: 2339.0056s\n",
      "[Test #16] Loss: 0.4371 Acc: 91.8598% Time: 2348.9736s\n",
      "Epoch 17 running\n",
      "[Train #17] Loss: 0.3664 Acc: 93.3367% Time: 2478.0078s\n",
      "[Test #17] Loss: 0.4119 Acc: 92.2023% Time: 2487.8802s\n",
      "Epoch 18 running\n",
      "[Train #18] Loss: 0.3575 Acc: 93.3507% Time: 2616.3221s\n",
      "[Test #18] Loss: 0.4447 Acc: 91.4120% Time: 2626.1773s\n",
      "Epoch 19 running\n",
      "[Train #19] Loss: 0.3463 Acc: 93.6064% Time: 2754.5714s\n",
      "[Test #19] Loss: 0.4759 Acc: 91.3857% Time: 2764.5190s\n",
      "Epoch 20 running\n",
      "[Train #20] Loss: 0.3392 Acc: 93.8157% Time: 2893.1019s\n",
      "[Test #20] Loss: 0.3494 Acc: 93.2824% Time: 2903.0018s\n",
      "Epoch 21 running\n",
      "[Train #21] Loss: 0.3308 Acc: 93.7831% Time: 3031.1805s\n",
      "[Test #21] Loss: 0.3769 Acc: 92.8609% Time: 3041.1608s\n",
      "Epoch 22 running\n",
      "[Train #22] Loss: 0.3230 Acc: 94.0528% Time: 3169.7335s\n",
      "[Test #22] Loss: 0.3572 Acc: 93.0980% Time: 3179.8600s\n",
      "Epoch 23 running\n",
      "[Train #23] Loss: 0.3129 Acc: 94.2528% Time: 3308.6808s\n",
      "[Test #23] Loss: 0.3433 Acc: 93.6249% Time: 3318.5391s\n",
      "Epoch 24 running\n",
      "[Train #24] Loss: 0.3152 Acc: 94.0342% Time: 3446.6374s\n",
      "[Test #24] Loss: 0.3362 Acc: 93.5722% Time: 3456.4918s\n",
      "Epoch 25 running\n",
      "[Train #25] Loss: 0.2929 Acc: 94.5643% Time: 3585.0100s\n",
      "[Test #25] Loss: 0.3474 Acc: 93.5722% Time: 3595.1268s\n",
      "Epoch 26 running\n",
      "[Train #26] Loss: 0.2917 Acc: 94.6945% Time: 3723.5341s\n",
      "[Test #26] Loss: 0.3008 Acc: 94.3888% Time: 3733.4211s\n",
      "Epoch 27 running\n",
      "[Train #27] Loss: 0.2794 Acc: 94.7085% Time: 3862.2318s\n",
      "[Test #27] Loss: 0.2938 Acc: 94.2834% Time: 3872.2208s\n",
      "Epoch 28 running\n",
      "[Train #28] Loss: 0.2735 Acc: 94.9363% Time: 4000.5661s\n",
      "[Test #28] Loss: 0.3320 Acc: 93.5458% Time: 4010.6707s\n",
      "Epoch 29 running\n",
      "[Train #29] Loss: 0.2661 Acc: 95.0897% Time: 4138.6049s\n",
      "[Test #29] Loss: 0.3305 Acc: 93.5458% Time: 4148.4568s\n",
      "Epoch 30 running\n",
      "[Train #30] Loss: 0.2685 Acc: 94.9735% Time: 4276.9567s\n",
      "[Test #30] Loss: 0.3491 Acc: 92.9926% Time: 4286.9485s\n",
      "Epoch 31 running\n",
      "[Train #31] Loss: 0.2474 Acc: 95.3455% Time: 4415.7087s\n",
      "[Test #31] Loss: 0.4050 Acc: 92.0443% Time: 4425.6917s\n",
      "Epoch 32 running\n",
      "[Train #32] Loss: 0.2532 Acc: 95.2757% Time: 4553.6195s\n",
      "[Test #32] Loss: 0.3276 Acc: 93.4405% Time: 4563.6001s\n",
      "Epoch 33 running\n",
      "[Train #33] Loss: 0.2493 Acc: 95.3966% Time: 4692.2183s\n",
      "[Test #33] Loss: 0.2818 Acc: 94.6259% Time: 4702.2524s\n",
      "Epoch 34 running\n",
      "[Train #34] Loss: 0.2409 Acc: 95.5454% Time: 4830.5219s\n",
      "[Test #34] Loss: 0.2574 Acc: 94.9420% Time: 4840.4381s\n",
      "Epoch 35 running\n",
      "[Train #35] Loss: 0.2327 Acc: 95.6803% Time: 4968.4678s\n",
      "[Test #35] Loss: 0.2615 Acc: 94.8894% Time: 4978.7071s\n",
      "Epoch 36 running\n",
      "[Train #36] Loss: 0.2343 Acc: 95.5780% Time: 5107.2464s\n",
      "[Test #36] Loss: 0.2377 Acc: 95.3372% Time: 5117.1442s\n",
      "Epoch 37 running\n",
      "[Train #37] Loss: 0.2264 Acc: 95.8663% Time: 5245.7580s\n",
      "[Test #37] Loss: 0.2656 Acc: 94.9420% Time: 5255.7646s\n",
      "Epoch 38 running\n",
      "[Train #38] Loss: 0.2214 Acc: 95.9732% Time: 5383.7573s\n",
      "[Test #38] Loss: 0.2565 Acc: 94.7313% Time: 5393.7420s\n",
      "Epoch 39 running\n",
      "[Train #39] Loss: 0.2185 Acc: 95.9128% Time: 5522.6238s\n",
      "[Test #39] Loss: 0.3448 Acc: 93.8620% Time: 5532.5753s\n",
      "Epoch 40 running\n",
      "[Train #40] Loss: 0.2121 Acc: 95.9872% Time: 5661.7143s\n",
      "[Test #40] Loss: 0.2978 Acc: 94.3361% Time: 5671.7105s\n",
      "Epoch 41 running\n",
      "[Train #41] Loss: 0.2123 Acc: 96.0476% Time: 5800.4485s\n",
      "[Test #41] Loss: 0.2725 Acc: 94.9420% Time: 5810.4825s\n",
      "Epoch 42 running\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 50   #(set no of epochs)\n",
    "start_time = time.time() #(for showing time)\n",
    "for epoch in range(num_epochs): #(loop for every epoch)\n",
    "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()    #(training model)\n",
    "    running_loss = 0 #(set loss 0)\n",
    "    running_corrects = 0 \n",
    "    # load a batch data of images\n",
    "    for i, inputs in enumerate(train_dataloader):\n",
    "        image=inputs['pixel_values']\n",
    "        labels1=inputs['label1']\n",
    "        labels2=inputs['label2']\n",
    "        # move to GPU\n",
    "        image = image.to(device)\n",
    "        labels1 = labels1.to(device) \n",
    "        labels2 = labels2.to(device) \n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        pre_labels1,pre_labels2 = model(image)\n",
    "        # print(\"pre_labels\",pre_labels1,pre_labels2)\n",
    "        _, preds1 = torch.max(pre_labels1, 1)\n",
    "        _, preds2 = torch.max(pre_labels2, 1)\n",
    "        # print(\"type\",preds1.dtype,labels1.dtype)\n",
    "        # print(\"preds\",preds1,preds2)\n",
    "        # print(\"labels\",labels1,labels2)\n",
    "        loss1 = criterion1(pre_labels1,labels1)\n",
    "        loss2=criterion2(pre_labels2,labels2)\n",
    "        # 两个分类loss之和\n",
    "        loss=loss1+loss2\n",
    "        # print(\"loss\",loss)\n",
    "        # get loss value and update the network weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        # print(\"size\",pre_labels1.size(),labels1.size())\n",
    "        # print(\"data\",labels1.data)\n",
    "        # 两个分类的正确数\n",
    "        running_corrects += torch.sum(preds1 == labels1.data)\n",
    "        running_corrects += torch.sum(preds2 == labels2.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / (len(train_dataset)*2) * 100.\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
    "    \n",
    "    # \"\"\" Testing Phase \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        for index, inputs  in enumerate(test_dataloader):\n",
    "            image, labels1,labels2 = inputs['pixel_values'],inputs['label1'],inputs['label2']\n",
    "            image = image.to(device)\n",
    "            labels1= labels1.to(device)\n",
    "            labels2= labels2.to(device)\n",
    "            outputs1,outputs2 = model(image)\n",
    "            _, preds1 = torch.max(outputs1, 1)\n",
    "            _, preds2 = torch.max(outputs2, 1)\n",
    "\n",
    "# \n",
    "            loss1 = criterion1(outputs1, labels1)\n",
    "            loss2 = criterion2(outputs2, labels2)\n",
    "            loss = loss1 + loss2\n",
    "            running_loss += loss.item() * image.size(0)\n",
    "\n",
    "            running_corrects += torch.sum(preds1 == labels1.data)\n",
    "            running_corrects += torch.sum(preds2 == labels2.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(test_dataset)\n",
    "        epoch_acc = running_corrects / (2*len(test_dataset))* 100\n",
    "        print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'dataset2-50-vit_unfreeze_pretrained.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= DualVisionTransformer(2,3)\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds2 \u001b[38;5;241m==\u001b[39m labels2\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m         \u001b[43mplot_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataset)\n\u001b[1;32m     28\u001b[0m epoch_acc \u001b[38;5;241m=\u001b[39m running_corrects \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataset)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100.\u001b[39m\n",
      "File \u001b[0;32m~/DTViT/plot_image.py:13\u001b[0m, in \u001b[0;36mplot_image\u001b[0;34m(image, labels1, labels2)\u001b[0m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m})\n\u001b[1;32m     11\u001b[0m images \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(image[:\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m     12\u001b[0m title1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m real label: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m predicted label: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;241m.\u001b[39mformat([f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfilename\u001b[49m] ,[class_names1[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m labels1[:\u001b[38;5;241m4\u001b[39m]],[class_names1[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m preds[:\u001b[38;5;241m4\u001b[39m]])\n\u001b[1;32m     14\u001b[0m imshow(images\u001b[38;5;241m.\u001b[39mcpu(), title1)\n\u001b[1;32m     15\u001b[0m title2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m real label: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m predicted label: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mformat([f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filename] ,[class_names2[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m labels2[:\u001b[38;5;241m4\u001b[39m]],[class_names2[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m preds2[:\u001b[38;5;241m4\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "from plot_image import plot_image\n",
    "##Testing\n",
    "model.eval()\n",
    "model.to(device)\n",
    "start_time = time.time()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    for index, inputs  in enumerate(test_dataloader):\n",
    "        image, labels1,labels2 = inputs['pixel_values'],inputs['label1'],inputs['label2']\n",
    "        image = image.to(device)\n",
    "        labels1= labels1.to(device)\n",
    "        labels2= labels2.to(device)\n",
    "        outputs1,outputs2  = model(image)\n",
    "        _, preds1 = torch.max(outputs1, 1)\n",
    "        _, preds2 = torch.max(outputs2, 1)\n",
    "        loss1 = criterion1(outputs1, labels1)\n",
    "        loss2 = criterion2(outputs2, labels2)\n",
    "        loss = loss1 + loss2\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        running_corrects += torch.sum(preds1 == labels1.data)\n",
    "        running_corrects += torch.sum(preds2 == labels2.data)\n",
    "        if index == 0:\n",
    "            plot_image(image, labels1, labels2)\n",
    "    epoch_loss = running_loss / len(test_dataset)\n",
    "    epoch_acc = running_corrects / (2* len(test_dataset)) * 100.\n",
    "    print('[Test #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.\n",
    "          format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
